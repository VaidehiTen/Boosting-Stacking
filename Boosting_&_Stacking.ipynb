{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Boosting & Stacking Assignment**\n"
      ],
      "metadata": {
        "id": "YFRR-li3Ji9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners**.\n",
        "- Boosting is an ensemble machine learning technique that builds a strong predictive model by combining multiple \"weak learners\" in a sequential process.\n",
        "- A weak learner is any model that performs only slightly better than random guessing, such as a shallow decision tree (often called a decision stump). Unlike other ensemble methods like bagging, which train models independently, boosting trains each new model to correct the errors of its predecessor.\n",
        "- The final strong model is a weighted combination of all the weak learners. This iterative approach focuses on minimizing errors, primarily reducing bias and to a lesser extent, variance, which results in a highly accurate and robust model.\n",
        "\n",
        "  **How boosting improves weak learners**\n",
        "\n",
        "- Boosting improves weak learners by iteratively training new models that learn from the errors of their predecessors. This approach effectively reduces the model's bias and leads to a more robust and accurate final model.\n",
        "- Here is a step-by-step breakdown of the general process, using AdaBoost as an example:\n",
        "  - Initial model: A weak learner is trained on the dataset, with each data point having an equal weight.\n",
        "  - Identify and weigh errors: The model makes an initial prediction, and the algorithm identifies which data points were misclassified. It then increases the weight of these misclassified data points, giving them more importance.\n",
        "  - Train the next learner: A second weak learner is trained on the dataset, but this time it pays more attention to the higher-weighted, misclassified data points.\n",
        "  - Repeat and aggregate: This process repeats for a set number of iterations. Each subsequent model is trained to correct the errors of the combined ensemble that came before it.\n",
        "  - Final model: The final, \"strong\" model combines the results of all the weak learners. Learners that performed better are given more weight in the final prediction, resulting in a model that can make more accurate and complex predictions than any single learner could.\n",
        "\n"
      ],
      "metadata": {
        "id": "OPHuHxIVJo6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?**\n",
        "- AdaBoost and Gradient Boosting both train models sequentially to correct the mistakes of previous models, but they differ fundamentally in how they identify and address those errors.\n",
        "- AdaBoost modifies the weight of each data point to focus on misclassified samples, while Gradient Boosting trains new models to specifically predict the residual errors of the prior models.\n",
        "\n",
        "  **How AdaBoost trains its models**\n",
        "\n",
        "  The AdaBoost (Adaptive Boosting) algorithm uses a system of weighted votes to train its weak learners, which are typically shallow decision trees or \"stumps\".\n",
        "  - Initial weights: All training examples are initially given an equal weight.\n",
        "  - Sequential training: A weak learner is trained on the dataset using the current sample weights. After training, the algorithm evaluates the learner's performance.\n",
        "  - Update weights:\n",
        "    - The weights of correctly classified instances are decreased.\n",
        "    - The weights of misclassified instances are increased, so the next weak learner will focus more on these \"difficult\" cases.\n",
        "  - Repeat and combine: This process repeats for a set number of iterations. The final model is a weighted sum of all the weak learners, where the vote of each learner is weighted by its accuracy.\n",
        "\n",
        "  **How Gradient Boosting trains its models**\n",
        "\n",
        "  Gradient Boosting takes a more mathematical approach, using a differentiable loss function to find the errors that need to be corrected.\n",
        "  - Initial prediction: The process starts with a simple initial prediction, such as the average of the target values.\n",
        "  - Calculate residuals: The algorithm calculates the residuals, which are the differences between the actual and predicted values. For classification, it uses a variation called \"pseudo-residuals\".\n",
        "  - Train on residuals: A new weak learner, usually a deeper decision tree than an AdaBoost stump, is trained to predict the residuals from the previous step.\n",
        "  - Sequential addition: The new model is added to the ensemble, but its contribution is scaled down by a learning rate to prevent overfitting.\n",
        "  - Repeat and refine: The new ensemble prediction is the initial prediction plus the (scaled) output of the new model. The residuals are recalculated, and a new model is trained to fix the remaining errors. This continues until the overall error is minimized."
      ],
      "metadata": {
        "id": "WsaMsZrAQoXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **How does regularization help in XGBoost?**\n",
        "- Regularization helps XGBoost by controlling model complexity and preventing overfitting, which is when a model learns the training data too well and performs poorly on unseen data. This leads to a more robust and generalized model that can make accurate predictions on new data.\n",
        "\n",
        "  **How regularization works in XGBoost**\n",
        "\n",
        "  XGBoost's objective function is a combination of two parts: the training loss and a regularization term. The regularization term penalizes the complexity of the model, specifically the decision trees being built, to keep it from becoming too intricate.\n",
        "\n",
        "  The primary ways regularization is implemented include:\n",
        "-  L1 and L2 regularization (**α**) and (**λ**): These are parameters that penalize the \"leaf weights\" (the final prediction scores in each leaf of the tree).\n",
        "    - L1 regularization (**α**): Adds a penalty equal to the absolute value of the leaf weights, which can force some of them to become exactly zero. This promotes sparsity, making the model simpler by effectively removing less important features.\n",
        "    - L2 regularization (**λ**): Adds a penalty equal to the square of the leaf weights, which encourages the weights to be small but not necessarily zero. It stabilizes the model by preventing any single feature from having too much influence.\n",
        "- Gamma (**γ**): This parameter specifies the minimum loss reduction required to make a further split on a leaf node. A higher **γ** value makes the algorithm more conservative by creating fewer, more significant splits, which results in shallower trees and less risk of overfitting.\n",
        "- `max_depth`: This hyperparameter directly limits how deep each decision tree can grow. A shallower tree is less complex and less likely to overfit.\n",
        "- `min_child_weight`: This sets a minimum threshold on the sum of instance weights (or \"cover\") needed in a child node for a split to occur. Increasing this value makes the algorithm more conservative by preventing it from creating partitions on nodes with few data points, which are more susceptible to noise.\n",
        "- Subsampling (`subsample` and `colsample`): These techniques introduce randomness by training each new tree on only a fraction of the data or a subset of the features. This prevents the model from becoming dependent on specific data points or features, improving its robustness.\n",
        "- Early stopping (`early_stopping_rounds`): This technique stops the training process if a model's performance on a validation set doesn't improve for a specified number of rounds. This prevents the model from training for too long and overfitting.\n",
        "- Shrinkage (`eta` or `learning_rate`): The learning rate controls how much each new tree's contribution shrinks towards the final prediction. A smaller learning rate forces the model to take smaller steps, requiring more trees but improving its robustness and generalization.     "
      ],
      "metadata": {
        "id": "eObYPI4-SKdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Why is CatBoost considered efficient for handling categorical data?**\n",
        "- CatBoost is efficient at handling categorical data primarily because of its ordered target encoding and ordered boosting mechanisms, which prevent the data leakage and overfitting that can plague other gradient boosting models. This approach bypasses the need for manual, time-consuming preprocessing steps like one-hot encoding.\n",
        "\n",
        "  **How CatBoost's categorical handling works**\n",
        "\n",
        "- Ordered Target Encoding\n",
        "\n",
        "  Standard target encoding, which replaces categories with the mean of the target variable, is susceptible to a form of data leakage called \"target leakage\". If the model uses the target variable to encode the same data it is trained on, it can learn patterns from the target variable that would not be available for real-world predictions.\n",
        "\n",
        "  CatBoost avoids this by using a permutation-driven approach to compute target statistics:\n",
        "  - It creates a random permutation of the training data.\n",
        "  - For each example in the dataset, it calculates the target statistic based only on the rows that appeared before it in the random permutation.\n",
        "  - This ensures the model cannot use information from future data points to encode the current one, protecting against data leakage and overfitting.\n",
        "- Ordered Boosting\n",
        "\n",
        "  Classic gradient boosting algorithms can suffer from a \"prediction shift\" because the gradients for a new tree are calculated using the same data the current model was trained on. CatBoost addresses this with an ordered boosting mechanism.\n",
        "  - In each iteration, CatBoost builds a new base model on a different, randomly permuted subset of the data.\n",
        "  - The residuals (errors) for a data point are calculated using a model trained only on data points that came before it in the permutation.\n",
        "  - This reduces bias in gradient estimation, which in turn leads to more robust and accurate models.  \n",
        "\n"
      ],
      "metadata": {
        "id": "X1YbZjHVVOSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?**\n",
        "- Boosting techniques are often preferred over bagging in applications where achieving high predictive accuracy on complex, imbalanced, or intricate datasets is the top priority. Unlike bagging (which focuses on reducing variance), boosting sequentially builds models that correct the errors of previous ones, effectively reducing bias and uncovering subtle data patterns.\n",
        "\n",
        "  **Real-world applications for boosting over bagging**\n",
        "\n",
        "- Medical diagnosis\n",
        "\n",
        "  Boosting is used for tasks like predicting disease risk or classifying diseases from patient data.\n",
        "  - Boosting's strength: When a disease is rare, boosting can learn from the errors of a weak classifier to improve its ability to identify the complex patterns associated with the minority class.\n",
        "  - Bagging's weakness: In a highly imbalanced dataset, a bagging model may overlook the patterns associated with the rare disease, as its focus is averaged across all cases, potentially leading to misdiagnosis.\n",
        "- Marketing and customer analytics\n",
        "  \n",
        "  Boosting is ideal for precise marketing efforts, such as predicting customer churn or customer lifetime value.\n",
        "  - Boosting's strength: These models excel at finding the subtle behaviors and features that indicate a customer is at high risk of leaving, allowing for targeted intervention strategies.\n",
        "  - Bagging's weakness: Since it averages the results of multiple independent models, a bagging model may not have the same precision for predicting rare, complex events like churn, especially if the signal is subtle.  \n",
        "- Fraud detection\n",
        "\n",
        "  In finance and e-commerce, fraud detection is a classic example of an imbalanced dataset, where fraudulent transactions are extremely rare compared to legitimate ones.\n",
        "  - Boosting's strength: By sequentially re-weighting misclassified transactions, boosting algorithms like XGBoost and LightGBM force the model to focus on the hard-to-classify, rare fraudulent cases.\n",
        "  - Bagging's weakness: A bagging method like Random Forest might average out the importance of these rare events because most of its base models are trained on resampled data dominated by legitimate transactions.   \n",
        "- Ranking systems\n",
        "\n",
        "  Search engines and recommender systems require a specific order of items rather than simple classification.\n",
        "  - Boosting's strength: Modern boosting libraries like LightGBM and XGBoost offer specialized ranking objective functions that can directly optimize for the correct order of items. By iteratively refining the model, they can learn the complex, non-linear relationships that determine relative relevance.\n",
        "  - Bagging's weakness: Standard bagging methods are not designed to optimize for ranked outcomes and are less effective at capturing the subtle, complex feature interactions required for accurate item ordering.   "
      ],
      "metadata": {
        "id": "nZKchzHdZC_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Write a Python program to:**\n",
        "  - **Train an AdaBoost Classifier on the Breast Cancer dataset**\n",
        "  - **Print the model accuracy**"
      ],
      "metadata": {
        "id": "Yq8JRIlvcKTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost classifier\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tz2MIh2ca8A",
        "outputId": "0cfc3009-f9fa-40f6-90b6-96c1aebe8f4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Write a Python program to:**\n",
        "  - **Train a Gradient Boosting Regressor on the California Housing dataset**\n",
        "  - **Evaluate performance using R-squared score**"
      ],
      "metadata": {
        "id": "EemcKn1xdKRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Gradient Boosting Regressor R-squared Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em5UEvojdWgS",
        "outputId": "189a660d-90f6-4d89-e623-9547629698d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.7756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Write a Python program to:**\n",
        "  - **Train an XGBoost Classifier on the Breast Cancer dataset**\n",
        "  - **Tune the learning rate using GridSearchCV**\n",
        "  - **Print the best parameters and accuracy**"
      ],
      "metadata": {
        "id": "SKuNZtAUdhL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSRqAKHhdyyg",
        "outputId": "cd2e0f7c-193c-4d38-b157-fb87d56f8747"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the XGBoost Classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Set up the parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to tune the learning rate\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Accuracy of Best Model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ETTDOh7d_wf",
        "outputId": "77b1bda8-57c5-4268-8fb5-a6e92203727f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Accuracy of Best Model: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [11:37:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **Write a Python program to:**\n",
        " -  **Train a CatBoost Classifier**\n",
        " -  **Plot the confusion matrix using `seaborn`**"
      ],
      "metadata": {
        "id": "AJVSsq_9eL2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost seaborn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geGWSGJFeuvF",
        "outputId": "7ff82b3a-f745-4b20-d213-49a92a7fe0f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=500, learning_rate=0.1, depth=6, silent=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Malignant', 'Benign'], yticklabels=['Malignant', 'Benign'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "l3Wpj8skede2",
        "outputId": "7610e7b7-330e-4531-8018-da5cecd6c0f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:**\n",
        "- **Data preprocessing & handling missing/categorical values**\n",
        "- **Choice between AdaBoost, XGBoost, or CatBoost**\n",
        "- **Hyperparameter tuning strategy**\n",
        "- **Evaluation metrics you'd choose and why**\n",
        "- **How the business would benefit from your model**\n",
        "\n",
        "\n",
        "- To develop a robust loan default prediction model for a FinTech company, the data science pipeline would systematically address challenges like imbalanced data, missing values, and mixed feature types. A boosting ensemble model, such as CatBoost, XGBoost, or AdaBoost, can then be trained and fine-tuned to maximize predictive power.\n",
        "\n",
        "  **Step-by-step data science pipeline**\n",
        "  \n",
        "  **Data preprocessing & handling missing/categorical values**\n",
        "- Exploratory Data Analysis (EDA): Understand the dataset by examining summary statistics, distributions, and the relationship between features. Key checks include identifying the proportion of defaults, understanding the nature of features (numeric, categorical), and visualizing the data distribution.\n",
        "- Addressing class imbalance: The dataset contains significantly more non-defaulting customers than defaulting ones. To prevent the model from becoming biased towards the majority class, one of the following resampling techniques should be applied to the training data:\n",
        "  - SMOTE (Synthetic Minority Over-sampling Technique): Creates synthetic data points for the minority (default) class to balance the class distribution.\n",
        "  - Class weighting: The model's loss function can be adjusted to give a higher penalty for misclassifying the minority class, which is a method offered by most boosting algorithms.\n",
        "  - NearMiss Undersampling: Randomly removes samples from the majority class to balance the dataset.\n",
        "- Handling missing values: Missing data must be handled appropriately to prevent model errors.\n",
        "  - For numerical features: Use imputation techniques like replacing missing values with the mean, median, or mode. For a more sophisticated approach, use K-Nearest Neighbors (KNN) imputation, which estimates missing values based on similar data points.\n",
        "  - For categorical features: Fill missing values with the mode. An alternative is to create a new category for \"missing\" if the missingness is non-random and holds predictive power.\n",
        "- Encoding categorical features: Most boosting algorithms require numerical inputs.\n",
        "  - Target-agnostic: Use techniques like one-hot encoding for nominal features, or label encoding for ordinal features. This approach is often used with AdaBoost and XGBoost.\n",
        "  - Native handling: Use CatBoost's built-in capability to handle categorical features directly, which can improve model performance and reduce preprocessing efforts.  \n",
        "\n",
        "  **Choice between AdaBoost, XGBoost, or CatBoost**\n",
        "\n",
        "  The choice of boosting algorithm depends on the dataset's characteristics and the specific requirements of the project.\n",
        "- AdaBoost:\n",
        "  - Best for: Small, clean datasets and fast training.\n",
        "  - Considerations: Less powerful than gradient boosting methods for complex problems. Does not natively handle missing values or categorical features, requiring careful preprocessing.\n",
        "- XGBoost:\n",
        "  - Best for: Large datasets, as it is highly scalable, and situations requiring regularization to prevent overfitting.\n",
        "  - Considerations: Faster than traditional Gradient Boosting. Does not natively handle categorical features, so they must be pre-encoded. Offers high performance and robust feature handling.\n",
        "- CatBoost:\n",
        "  - Best for: Datasets with many categorical features, as it handles them natively, and datasets with outliers, as it is more robust to them.\n",
        "  - Considerations: Often produces high accuracy out-of-the-box but can be slower to train than XGBoost or LightGBM. Its ordered boosting approach helps prevent overfitting on categorical data.  \n",
        "\n",
        "  **Hyperparameter tuning strategy**\n",
        "\n",
        "  To find the optimal hyperparameters for the chosen boosting model, the following strategy should be employed:\n",
        "- Define the search space: Specify a range of values for critical hyperparameters, such as learning rate, number of estimators, and tree depth.\n",
        "- Use cross-validation: Split the training data into k-folds to ensure robust evaluation and prevent data leakage. Stratified K-fold cross-validation is essential here to maintain the class distribution in each fold.\n",
        "- Choose a search method:\n",
        "  - Randomized Search with Cross-Validation (RandomizedSearchCV): Search a specified number of parameter combinations randomly. This is more computationally efficient than grid search and often finds a good solution faster.\n",
        "  - Bayesian Optimization: Use past results to intelligently select the next set of hyperparameters to try. This is more efficient for large search spaces and can find optimal solutions in fewer iterations.\n",
        "- Consider cost-sensitive tuning: Given the business impact of false negatives (approving a defaulting loan) versus false positives (denying a non-defaulting loan), the tuning process can incorporate cost-sensitive metrics.\n",
        "\n",
        "  **Evaluation metrics you'd choose and why**\n",
        "\n",
        "  In an imbalanced dataset, accuracy is misleading because a model can achieve high accuracy by simply predicting the majority class. The following metrics provide a more nuanced and business-relevant evaluation:\n",
        "- Confusion Matrix: A detailed breakdown of True Positives, True Negatives, False Positives, and False Negatives, which helps interpret the model's error types.\n",
        "- Precision and Recall:\n",
        "  - Precision: The ratio of true positives to the total predicted positives. It measures the model's accuracy in predicting defaults. For a FinTech, this is important for managing risk and ensuring that predicted defaults are accurate.\n",
        "  - Recall (Sensitivity): The ratio of true positives to the total actual positives. It measures the model's ability to find all defaulters. Maximizing recall is often critical in loan default prediction to catch as many at-risk borrowers as possible.\n",
        "- F1-Score: The harmonic mean of precision and recall. It offers a balanced measure of a model's performance on the minority class.\n",
        "- AUC-ROC (Area Under the Receiver Operating Characteristic Curve): Measures the model's ability to discriminate between positive (default) and negative (non-default) classes across various probability thresholds. A high AUC-ROC indicates strong discriminatory power.\n",
        "- AUC-PR (Area Under the Precision-Recall Curve): Particularly useful for highly imbalanced datasets, as it focuses on the performance of the minority class. A higher AUC-PR indicates better performance on the positive (default) class.\n",
        "\n",
        "  **How the business would benefit from your model**\n",
        "\n",
        "  This predictive model offers several tangible benefits for the FinTech company:\n",
        "- Improved risk management: By accurately predicting the probability of default, the company can make more informed lending decisions. It can set appropriate interest rates and terms based on risk scores, or deny high-risk applications, thereby reducing financial losses from defaults.\n",
        "- Enhanced profitability: With better risk assessment, the company can expand its lending pool to creditworthy individuals who might have been previously rejected by traditional scoring methods, increasing its customer base and profitability.\n",
        "- Early intervention: The model can provide early warning signals for existing loans that show high-risk behavior. This allows the company to proactively engage with these customers and implement strategies like loan restructuring or refinancing to prevent a default from occurring.\n",
        "- Operational efficiency: The automated and data-driven approach to risk assessment can speed up the loan application process, reducing manual underwriting efforts and operational costs.\n",
        "- Fairness and interpretability: Using modern boosting models like CatBoost and employing Explainable AI techniques like SHAP allows for insights into which features contribute most to the prediction. This helps ensure the model is not relying on biased or unfair features, which is critical for regulatory compliance and building customer trust.\n",
        "\n"
      ],
      "metadata": {
        "id": "cge_LzvMe6O6"
      }
    }
  ]
}